# Copilot Instructions: Simple Multi-Agent CrewAI Test Setup (Using PostgreSQL) - Final Version (Maximum Context)

## 1. Overall Goal & High-Level Context (Comprehensive)

**Primary Objective:** Your fundamental purpose is to generate **three distinct, simplified Python test harnesses**. These are *not* the final application but rather controlled environments designed to meticulously validate foundational interactions *before* attempting to run the main, complex application located in `src/ollama/`. The harnesses will verify:
    1.  A baseline multi-agent workflow using **Gemini only**.
    2.  The identical workflow using **LM Studio only**.
    3.  A collaborative workflow involving **both Gemini and LM Studio agents**.

**Critical Constraint:** All test harnesses **must** successfully log basic execution parameters and a success/failure metric to a pre-configured **PostgreSQL** database via MLflow, using direct `mlflow` library calls.

**Why this Phased Approach? (Elaborated Rationale):**
The target system (`src/ollama/`) is architecturally complex. It aims to dynamically orchestrate multiple LLMs (cloud-based Gemini, local LM Studio), integrate a diverse toolset (including standard web search, custom XML/LangGraph-based structured thinking, code execution), manage a knowledge base, and perform sophisticated monitoring via a custom `MLflowDashboard` utility â€“ all driven by extensive YAML configuration.

Attempting a "big bang" test of such a system is statistically likely to result in numerous, simultaneous failures across different subsystems (API connectivity, configuration parsing, tool execution, database connection, custom logic errors, dependency mismatches). Debugging this interconnected web of potential failures is extremely time-consuming and inefficient.

These simple test harnesses mitigate this risk by **deconstructing the problem**. We isolate and verify the most fundamental interactions first:
    *Can we connect to *each* LLM provider independently?
    *   Can `crewai` execute a simple sequence of tasks with context passing?
    *Can `crewai` handle agents using *different* LLM backends in the same workflow (critical for the combo test)?
    *   Can the application connect to and perform basic logging operations on the **PostgreSQL** MLflow backend? (Mandatory backend choice).
    *   Can the `ToolFactory` instantiate at least one basic, external-dependency tool (`web_search`)?

Only after confirming these core building blocks function correctly in isolation will we proceed to integrate them into the main application structure, significantly reducing the number of variables when debugging the more complex features later.

**Key Technologies & Assumptions (Deep Dive for Copilot):**

* **`crewai` Framework (v0.108.0+ likely based on `pyproject.toml`):**
  * `Agent`: Key inputs are `role`, `goal`, `backstory`, `llm` (the LangChain wrapper instance), `tools` (list of *instances*). `verbose=True` provides agent thought process logs; `allow_delegation=False` prevents unexpected agent-to-agent calls in these simple tests.
  * `Task`: Key inputs are `description`, `expected_output`, `agent` (the `Agent` object). The `context=[previous_task_object]` list is the *explicit mechanism* under test for passing the output of one task as implicit input/context to the next in a `Process.sequential` crew. Your code *must* implement this linking correctly.
  * `Crew`: Takes `agents=[...]`, `tasks=[...]`. Use `process=Process.sequential` *only*. `verbose=2` provides the most detailed logs, essential for verifying context passing and tool usage attempts.
* **LangChain LLM Wrappers (Interfaces):**
  * `langchain_google_genai.ChatGoogleGenerativeAI`: Needs `google_api_key=os.getenv("GEMINI_API_KEY")` and `model=os.getenv("GEMINI_MODEL")`. Assume these env vars are set. Code *must* check for the key.
  * `langchain_openai.ChatOpenAI`: Needs `openai_api_base=derived_url`, `openai_api_key="not-needed"`, `model_name=os.getenv("LMSTUDIO_MODEL")`. The `derived_url` *must* come from `os.getenv("LMSTUDIO_API_URL")`, and your code *must* check if it ends with `/v1` and append it if missing, as this is standard for OpenAI compatibility. Code *must* check for the base URL env var.
* **`ToolFactory` (`src.ollama.tools.tool_factory.ToolFactory`):**
  * **Location/Function Assumption:** Exists at the specified path and has a `get_tool(name: str)` method that returns an initialized tool instance based on configuration likely residing in `src/ollama/config/tools.yaml`.
  * **`"web_search"` Tool Context:** This is the *only* tool we will test via the factory. Its successful initialization likely depends on `os.getenv("SERPER_API_KEY")` being valid for the underlying `SerperDevTool`.
  * **Mandatory Error Handling:** Wrap the `get_tool("web_search")` call in a comprehensive `try...except (KeyError, Exception) as e:` block. If an error occurs, `print` a detailed warning message including the exception `e` and explicitly mention the possibility of a missing/invalid `SERPER_API_KEY`. Crucially, set the tool variable to `None` in the `except` block so the test can continue, verifying the rest of the flow even if the tool is unavailable.
* **MLflow & PostgreSQL Integration (Critical Context):**
  * **Backend Choice:** **PostgreSQL is non-negotiable** for these tests and the final application due to anticipated concurrency needs. The `MLFLOW_TRACKING_URI` environment variable *must* point to a valid PostgreSQL connection string (e.g., `postgresql://user:pass@host:port/db`).
  * **Schema Management:** **Emphasize: MLflow handles schema creation/migration automatically.** Your code only needs to ensure the `MLFLOW_TRACKING_URI` is correctly set via `mlflow.set_tracking_uri()`. No manual database DDL is required. The user is responsible only for ensuring the PG server is running, the target database exists, and the specified user has connection and creation privileges.
  * **Logging Scope (Basic Only):** Confine MLflow interactions to direct calls: `mlflow.set_tracking_uri`, `mlflow.set_experiment`, `mlflow.start_run`, `mlflow.log_param`, `mlflow.log_metric` (only `success`), `mlflow.log_text` (only result snippet), `mlflow.set_tag` (only for errors), `mlflow.end_run`. **Strictly avoid using the application's custom `MLflowDashboard` utility.**
  * **Robust Setup/Teardown:** The main test scripts *must* handle MLflow setup (`set_tracking_uri`, `set_experiment`) within a `try...except` block, exiting gracefully on failure. The core crew execution *must* be wrapped in `try...except...finally`, ensuring `mlflow.end_run()` is *always* called within the `finally` block *if and only if* `mlflow.start_run()` was successful.
* **Environment Variables (`.env` & `dotenv`):**
  * Mandate `from dotenv import load_dotenv; load_dotenv()` at the start of scripts needing env vars.
  * Consistently use `os.getenv("VAR_NAME", "optional_default")`. Perform explicit checks (`if not var: raise ValueError(...)` or log error/exit) for absolutely critical variables: `GEMINI_API_KEY` (for Gemini tests), `LMSTUDIO_API_URL` (for LM Studio tests), `MLFLOW_TRACKING_URI` (for all tests). `SERPER_API_KEY` is needed for the search tool but tests should proceed with a warning if it's missing.
* **Testing Focus (Recap):** Confirm: LLM connectivity (Gemini API, LM Studio endpoint), basic CrewAI agent execution, sequential task flow, `context` passing (including cross-model), basic `ToolFactory` operation with `web_search` (and graceful failure), and fundamental MLflow logging to PostgreSQL.
* **Exclusions (Recap):** **Do not** implement/test: advanced memory, model fallback, XML/LangGraph, code execution, `MLflowDashboard`, loading agent/task configs from YAML, parallel execution.
* **File Structure & Naming (Recap):** Strictly use **new files** in `src/ollama/` (`_multi_gemini_`, `_multi_lmstudio_`, `_combo_`). Ensure Python imports are correct relative to the project root.

---

## Phase 1: Simple Multi-Agent Gemini Test Harness (PostgreSQL Logging)

**Purpose:** Establish a working baseline using only Gemini. Verifies Gemini API auth/connection, CrewAI's ability to run a simple sequential workflow, context passing between tasks using the *same* LLM backend, optional basic tool usage via ToolFactory, and confirms the PostgreSQL MLflow logging pipeline is functional.

1. **Generate New Files:**
    * `src/ollama/main_multi_gemini_test.py`
    * `src/ollama/multi_gemini_crew.py`

2. **Generate `src/ollama/multi_gemini_crew.py`:**
    * *(Generate Python code based on the detailed steps from the previous "Maximum Context" response. Ensure meticulous implementation of imports, class structure, LLM instantiation with API key check, ToolFactory usage with robust error handling for `web_search`/`SERPER_API_KEY`, definition of 3 agents all using `gemini_llm`, definition of 3 tasks with explicit `context=[previous_task]` linking, and the standard `run` method.)*

3. **Generate `src/ollama/main_multi_gemini_test.py`:**
    * *(Generate Python code based on the detailed steps from the previous "Maximum Context" response. Ensure meticulous implementation of imports, `load_dotenv`, logging setup, robust MLflow setup including `MLFLOW_TRACKING_URI` check and `try...except` for `set_tracking_uri`/`set_experiment`, the main `try...except...finally` block wrapping crew execution, correct `mlflow.start_run`, `log_param`, `log_metric`, `log_text`, `set_tag`, and `end_run` calls, and final result printing.)*

4. **Run & Verify (Gemini):**
    * **Action:** From the project root, execute: `python -m src.ollama.main_multi_gemini_test`.
    * **Verification Checklist (Detailed):**
        * [ ] **Console - Execution:** Does the script complete without any Python exceptions or tracebacks?
        * [ ] **Console - Gemini API:** Do logs indicate successful authentication and responses from the Gemini API? (Look for absence of 4xx/5xx errors related to Google APIs).
        * [ ] **Console - Task Order:** Do `verbose=2` logs clearly show tasks starting and finishing in the sequence: `research_task` -> `summarize_task` -> `report_task`?
        * [ ] **Console - Context Pass (R->S):** Find the final output log for `research_task`. Find the initial input/prompt log for `summarize_task`. Does the summarizer's input explicitly contain or refer to the key findings produced by the researcher?
        * [ ] **Console - Context Pass (S->R):** Find the final output log for `summarize_task` (likely bullet points). Find the initial input/prompt log for `report_task`. Does the reporter's input explicitly contain or refer to the summary points?
        * [ ] **Console - Tool Usage:** If `SERPER_API_KEY` was valid, do logs show the `researcher` invoking the `web_search` tool and receiving results? If the key was invalid/missing, was the specific warning printed during initialization, and did the researcher proceed using only its internal knowledge?
        * [ ] **MLflow UI (PG) - Connectivity:** Can you access the MLflow UI, and does it show connectivity to your PostgreSQL backend?
        * [ ] **MLflow UI (PG) - Experiment:** Does the "Simple_Gemini_Test" experiment exist?
        * [ ] **MLflow UI (PG) - Run Creation:** Is a new run present in the experiment, corresponding to this execution?
        * [ ] **MLflow UI (PG) - Parameters:** Are the parameters `topic`, `model_provider="Gemini"`, and `crew_type="multi-agent-simple"` logged correctly for the run?
        * [ ] **MLflow UI (PG) - Metrics:** Is the `success` metric logged as `1.0`? (If an exception occurred, it should be `0.0` and have an `error` tag).
        * [ ] **MLflow UI (PG) - Artifacts:** Does the `final_result_snippet.txt` artifact exist under the run?

---

## Phase 2: Simple Multi-Agent LM Studio Test Harness (PostgreSQL Logging)

**Purpose:** Replicate the Phase 1 test using only LM Studio. Validates connection to the local LM Studio endpoint, function of the `ChatOpenAI` wrapper with CrewAI, context passing using this backend, and confirms PG logging works identically.

1. **Generate New Files:**
    * `src/ollama/main_multi_lmstudio_test.py`
    * `src/ollama/multi_lmstudio_crew.py`

2. **Generate `src/ollama/multi_lmstudio_crew.py`:**
    * **Adaptation:** Based on `multi_gemini_crew.py`.
    * **Imports:** Use `langchain_openai.ChatOpenAI`.
    * **Class `LMStudioMultiCrew`:**
        * `__init__`: Check `LMSTUDIO_API_URL`, perform `/v1` suffix logic. Instantiate `ChatOpenAI` (`self.lmstudio_llm`). Log instantiation.
        * `_create_agents_and_tasks`: **Critical: Ensure all 3 `Agent` instances use `llm=self.lmstudio_llm`**. Update role names slightly ('...(LM Studio)'). Tool logic (optional `web_search`) is identical. Task definitions and `context` links are identical.
        * `run`: Identical.
    * *(Generate Python code based on these detailed steps, ensuring correct LLM wrapper and assignment)*

3. **Generate `src/ollama/main_multi_lmstudio_test.py`:**
    * **Adaptation:** Based on `main_multi_gemini_test.py`.
    * **Imports:** Import `LMStudioMultiCrew`.
    * **MLflow Setup:** Change `experiment_name` to `"Simple_LMStudio_Test"`. MLflow URI handling remains the same.
    * **Main Block:** Update logging messages ("LM Studio Test..."). Log `model_provider="LM Studio"`. Instantiate `LMStudioMultiCrew`. Exception/finally blocks remain the same.
    * *(Generate Python code based on these detailed steps, ensuring correct class, experiment name, and parameter logging)*

4. **Run & Verify (LM Studio):**
    * **Prerequisite:** LM Studio running, correct model loaded, API server active.
    * **Action:** Execute `python -m src.ollama.main_multi_lmstudio_test`.
    * **Verification Checklist (Detailed):**
        * [ ] **Console - Execution:** Script completes without Python errors?
        * [ ] **Console - LM Studio API:** Logs show successful `POST` requests to the configured local URL (e.g., `http://localhost:1234/v1/chat/completions`)? No connection refused or 4xx/5xx errors from the local server?
        * [ ] **LM Studio Server Logs (Recommended):** Check LM Studio's console/logs to confirm it received requests and generated responses.
        * [ ] **Console - Task Order:** Research -> Summarize -> Report sequence maintained?
        * [ ] **Console - Context Pass (R->S):** Evidence `summarize_task` received/used `research_task` output?
        * [ ] **Console - Context Pass (S->R):** Evidence `report_task` received/used `summarize_task` output?
        * [ ] **Console - Tool Usage:** Same checks as Phase 1 regarding the optional `web_search` tool.
        * [ ] **MLflow UI (PG) - Experiment:** "Simple_LMStudio_Test" experiment exists?
        * [ ] **MLflow UI (PG) - Run Creation:** New run present?
        * [ ] **MLflow UI (PG) - Parameters:** Correct `topic`, `model_provider="LM Studio"`, `crew_type` logged?
        * [ ] **MLflow UI (PG) - Metrics:** `success` is `1.0`?
        * [ ] **MLflow UI (PG) - Artifacts:** `final_result_snippet.txt` exists?

---

## Phase 3: Simple Combined Model Test Harness (PostgreSQL Logging)

**Purpose:** The critical integration test. Verify a workflow where agents use **different LLMs (Gemini and LM Studio)**. This specifically tests CrewAI's core capability to pass context (`context=[...]`) effectively between agents regardless of their underlying `llm` instance type. Logs to PostgreSQL, identifying the model mix.

1. **Generate New Files:**
    * `src/ollama/main_combo_test.py`
    * `src/ollama/combo_crew.py`

2. **Generate `src/ollama/combo_crew.py`:**
    * **Imports:** Include *both* `ChatGoogleGenerativeAI` and `ChatOpenAI`.
    * **Class `ComboCrew`:**
        * `__init__`: Instantiate **both** `self.gemini_llm` (check key) **and** `self.lmstudio_llm` (check URL, append `/v1`). Log success for both instantiations. Instantiate `ToolFactory`. Call `_create_agents_and_tasks`.
        * `_create_agents_and_tasks`:
            * Get `web_search` tool (with `try...except`).
            * Define `self.researcher`: **`llm=self.gemini_llm`**. Role '...(Gemini)'.
            * Define `self.summarizer`: **`llm=self.lmstudio_llm`**. Role '...(LM Studio)'.
            * Define `self.reporter`: **`llm=self.gemini_llm`**. Role '...(Gemini)'. Log the specific mixed assignment pattern being used (e.g., "Assigning Gemini->LMStudio->Gemini").
            * Define tasks (`research_task`, `summarize_task`, `report_task`) **identically** to previous phases, maintaining `context` dependencies. Agent assignment uses the instances defined just above.
        * `run`: Identical logic.
    * *(Generate Python code based on these detailed steps, ensuring explicit mixed LLM assignment)*

3. **Generate `src/ollama/main_combo_test.py`:**
    * **Adaptation:** Based on `main_multi_gemini_test.py`.
    * **Imports:** Import `ComboCrew`.
    * **MLflow Setup:** Change `experiment_name` to `"Simple_Combo_Test"`.
    * **Main Block:** Update logging messages ("Combo Test..."). Log `crew_type="combo-multi-agent"`. **Add specific parameters logging the model for *each* role**: `mlflow.log_param("researcher_model", "Gemini")`, `mlflow.log_param("summarizer_model", "LM Studio")`, `mlflow.log_param("reporter_model", "Gemini")`. Instantiate `ComboCrew`.
    * *(Generate Python code based on these detailed steps, ensuring correct class, experiment name, and detailed parameter logging)*

4. **Run & Verify (Combo):**
    * **Prerequisite:** LM Studio server running with model loaded.
    * **Action:** Execute `python -m src.ollama.main_combo_test`.
    * **Verification Checklist (Detailed):**
        * [ ] **Console - Execution:** Script completes without Python errors?
        * [ ] **Console - Connectivity:** Logs show successful calls to *both* Gemini API *and* the LM Studio endpoint during the *same* run?
        * [ ] **Console - Task Order:** Research -> Summarize -> Report sequence maintained?
        * [ ] **Console - Cross-Model Context Pass (Gemini -> LM Studio):** **Critically examine `verbose=2` logs.** Does the input/prompt/context logged for the **LM Studio summarizer** contain the textual output generated by the **Gemini researcher**?
        * [ ] **Console - Cross-Model Context Pass (LM Studio -> Gemini):** **Critically examine `verbose=2` logs.** Does the input/prompt/context logged for the **Gemini reporter** contain the textual output generated by the **LM Studio summarizer**?
        * [ ] **MLflow UI (PG) - Experiment:** "Simple_Combo_Test" experiment exists?
        * [ ] **MLflow UI (PG) - Run Creation:** New run present?
        * [ ] **MLflow UI (PG) - Parameters:** Are `topic`, `crew_type`, and **all three specific `_model` role assignments** logged correctly?
        * [ ] **MLflow UI (PG) - Metrics:** `success` is `1.0`?
        * [ ] **MLflow UI (PG) - Artifacts:** `final_result_snippet.txt` exists?

---

## Final Next Steps: Incremental Integration into Main Application (Highly Detailed Path)

**Context Checkpoint:** Successfully completing all three phases provides strong, validated evidence for the core mechanics: individual LLM connectivity via LangChain wrappers, sequential task execution in CrewAI, effective context passing (even across different LLM backends), basic tool loading/invocation via ToolFactory, and reliable MLflow logging to PostgreSQL.

**Integration Strategy:** Now, merge these validated building blocks into the main application (`src/ollama/...`) **incrementally and methodically**. Test *rigorously* after each step.

1. **Step 1: Integrate Core LLM Instantiation & Basic Crew Structure:**
    * **Goal:** Make the main application capable of running the simple 3-step workflow using either Gemini *or* LM Studio, without YAML loading yet.
    * **Action 1.1 (LLM Init):** Focus on `src/ollama/crews/model_crews.py` (or related utils like `model_clients.py`). Ensure `GeminiCrew.__init__` correctly instantiates `ChatGoogleGenerativeAI` (with key check). Ensure `LMStudioCrew.__init__` correctly instantiates `ChatOpenAI` (with URL check, `/v1` logic, `key="not-needed"`). Add clear logging upon successful instantiation in both.
    * **Action 1.2 (Hardcode Simple Workflow):** In *both* `GeminiCrew._create_agents_and_tasks` and `LMStudioCrew._create_agents_and_tasks`, *temporarily remove* any existing complex logic or YAML loading. Directly hardcode the creation of the 3 agents (Researcher, Summarizer, Reporter), assigning the correct `llm` instance (`self.gemini_llm` or `self.lmstudio_llm`). Hardcode the 3 tasks, ensuring the `context=[...]` dependencies are explicitly set between them.
    * **Action 1.3 (Modify `main.py`):** Ensure `src/ollama/main.py` correctly parses `--model gemini` or `--model lmstudio`. Based on the argument, it should instantiate the corresponding `GeminiCrew` or `LMStudioCrew`. Ensure it calls the `crew.run()` method. Temporarily comment out calls to complex setup, validation, or advanced MLflow functions within `main.py`.
    * **Test 1:** Execute `python -m src.ollama.main --model gemini ...` AND `python -m src.ollama.main --model lmstudio ...`. **Verify:** Both commands run the simple 3-step workflow successfully using the *correct* LLM backend throughout, producing a final result. Debug issues related to class structure, LLM init, or basic `crew.run()` invocation within the main app context.

2. **Step 2: Integrate ToolFactory & Basic `web_search` Tool:**
    * **Goal:** Verify the main application can use the `ToolFactory` to provide the basic search tool to the researcher agent.
    * **Action 2.1 (ToolFactory Init):** Ensure `ToolFactory` is instantiated correctly within the relevant crew class (e.g., `BaseCrew` or `GeminiCrew`/`LMStudioCrew` `__init__`).
    * **Action 2.2 (Tool Assignment):** Modify the hardcoded `_create_agents_and_tasks` methods (from Step 1.2). Add the robust `try...except` block to get the `"web_search"` tool using `self.tool_factory.get_tool()`. Assign the result (tool instance or `None`) to the `tools` list of the hardcoded `researcher` agent.
    * **Test 2:** Rerun both `main.py` commands (Gemini/LM Studio). **Verify:** Check `verbose=2` logs. Does the researcher agent now attempt to use the `web_search` tool? If the tool failed init (due to missing key), is the warning logged, and does the crew still complete? Debug `ToolFactory` integration or tool assignment issues.

3. **Step 3: Integrate Basic MLflow Logging (PostgreSQL):**
    * **Goal:** Enable basic MLflow run tracking for the main application, logging to PostgreSQL.
    * **Action 3.1 (MLflow Setup in `main.py`):** Add necessary imports (`mlflow`, `os`, `dotenv`). Implement the MLflow setup block (get/check URI, `set_tracking_uri`, `set_experiment` based on `args.model`, `try...except` for robustness) near the start of the main execution logic.
    * **Action 3.2 (Wrap Execution):** In `main.py`, wrap the `crew.run()` call within the full `try...except...finally` structure. Include `mlflow.start_run()`, basic `mlflow.log_param()` (model, topic), `mlflow.log_metric("success", ...)` in try/except, `mlflow.log_text(...)` for snippet, and `mlflow.end_run()` in finally.
    * **Test 3:** Rerun both `main.py` commands. **Verify:** Check the MLflow UI (PG). Are runs created in the *correct* experiments (e.g., "gemini_crew_monitoring", "lmstudio_crew_monitoring")? Are the basic parameters and success metric logged? Debug MLflow setup or basic logging calls within `main.py`.

4. **Step 4: Integrate Basic Knowledge Management:**
    * **Goal:** Verify the main application can instantiate `KnowledgeManager` and perform a simple data retrieval.
    * **Action 4.1 (KM Init):** Instantiate `KnowledgeManager` appropriately within the main application structure (e.g., in `BaseCrew` or passed to the crew).
    * **Action 4.2 (Direct Test Call):** In `main.py`, add a temporary line *before* `crew.run()` to test a direct `knowledge_manager.get_entry('category', 'id')` call. Log the result or any errors.
    * **Action 4.3 (Tool Test - Optional):** If `KnowledgeBaseTool` was verified simply earlier, ensure `ToolFactory` provides it, assign it to a hardcoded agent, and modify a task to invoke it simply.
    * **Test 4:** Rerun `main.py`. **Verify:** Does the direct `get_entry` call succeed without path/parsing errors? If using the tool, does the agent attempt to invoke it? Debug basic `KnowledgeManager` instantiation or file access issues from the main app's working directory perspective.

5. **Step 5: Integrate Complex Tools (Systematically, One by One):**
    * **Goal:** Ensure each advanced/custom tool works correctly within the main application context.
    * **Action 5.1 (Select ONE Tool):** Choose *one* tool (e.g., `StructuredThinkingTool`).
    * **Action 5.2 (Config/Factory):** Verify its definition in `config/tools.yaml`. Ensure `ToolFactory` can instantiate it without errors. Add any necessary imports or dependencies this tool requires to the main application environment.
    * **Action 5.3 (Isolate Test):** Modify the *hardcoded* `_create_agents_and_tasks` in the relevant crew class. Assign *only this tool* to one agent. Modify that agent's task description to perform the simplest possible invocation of this tool (e.g., for `StructuredThinkingTool`, provide minimal valid XML content to validate). Temporarily simplify or comment out other tasks/agents if needed to focus the run.
    * **Test 5.A (Run & Debug):** Execute `main.py`. Focus exclusively on the logs related to this tool's invocation. Debug its internal logic, dependencies, input/output handling, and interaction with the agent/LLM within the main app context.
    * **Action 5.4 (Iterate):** Once Tool A works, *remove its specific invocation* from the task description (but leave the tool assigned if needed later). Select Tool B. Repeat 5.2-5.A for Tool B. Continue methodically for all critical custom/complex tools (`CodeExecutor`, `XML validators`, `LangGraph tools`, etc.).

6. **Step 6: Integrate Advanced MLflow (`MLflowDashboard`):**
    * **Goal:** Replace basic MLflow calls with the custom dashboard utility for comprehensive logging.
    * **Action 6.1 (Replace Calls):** In `main.py` (or wherever basic MLflow calls were added), remove them. Instantiate `MLflowDashboard` (likely needs config). Replace `mlflow.start_run` with `dashboard.start_run`, `mlflow.log_metric` with `dashboard.log_metrics`, etc. Ensure you pass the necessary data structures to the dashboard methods.
    * **Action 6.2 (Configure Dashboard):** Verify `config/mlflow_config.yaml` defines all the detailed metrics, thresholds, and visualization settings the `MLflowDashboard` utility expects.
    * **Test 6:** Rerun `main.py`. **Verify:** Check MLflow UI (PG). Are *all* the detailed metrics (performance, system, validation, model-specific) being logged? Are artifacts (plots) created if configured? Debug the `MLflowDashboard` utility itself, its configuration loading, metric calculation logic, and interaction with the `mlflow` library.

7. **Step 7: Integrate Advanced CrewAI Features:**
    * **Goal:** Implement and verify features like complex memory, model fallback, and parallelism.
    * **Action 7.1 (Memory):** Modify agent definitions (still potentially hardcoded, or start using YAML - see Step 8) to include the specific memory configurations designed (e.g., `ConversationBufferMemory` with specific limits, vector stores, custom memory types).
    * **Test 7.A (Memory):** Rerun `main.py`. Analyze `verbose=2` logs meticulously. Is memory being utilized? Is context being truncated/managed correctly based on the config? Does it improve/change agent behavior as expected?
    * **Action 7.2 (Fallback):** Implement the fallback mechanism (e.g., `ModelCoordinator`). Introduce controlled failures (e.g., temporarily wrong API key).
    * **Test 7.B (Fallback):** Rerun `main.py` with simulated failures. **Verify:** Does the system detect the primary model failure? Does it switch to the designated fallback model? Is this switch logged?
    * **Action 7.3 (Parallelism):** If applicable, change `process=Process.parallel` for the `Crew`.
    * **Test 7.C (Parallelism):** Rerun `main.py`. **Verify:** Performance improvement? Any deadlocks or race conditions introduced (check logs, observe hangs)? Ensure MLflow logging (especially if using the dashboard utility) handles potential concurrent updates correctly (this is why PG was chosen).

8. **Step 8: Integrate Full YAML Configuration Loading:**
    * **Goal:** Make the main application fully dynamic and configurable via YAML files, removing all hardcoded test workflows.
    * **Action 8.1 (Remove Hardcoding):** Eliminate the hardcoded agent/task definitions from `_create_agents_and_tasks`.
    * **Action 8.2 (Implement Loading Logic):** In `crew.py` or `main.py`, implement robust loading and parsing for `agents.yaml`, `tasks.yaml`, `tools.yaml`, `models.yaml`. This logic needs to:
        * Read the YAML files.
        * Dynamically instantiate `Agent` objects based on `agents.yaml`, assigning the correct LLM instance (based on `models.yaml` and potentially `model_assignments`) and tools (via `ToolFactory` based on `tools.yaml` and agent config). Handle inheritance if used in YAML.
        * Dynamically instantiate `Task` objects based on `tasks.yaml`, linking them to the created agent instances and setting `context` dependencies based on task relationships defined in YAML.
        * Pass the dynamically created lists of agents and tasks to the `Crew` constructor.
    * **Action 8.3 (Test Loading):** Rerun `main.py` using the *actual* complex configurations defined in your YAML files. Does it successfully build the intended crew without errors?
    * **Action 8.4 (Test Validation):** Thoroughly test the `validate_configuration` function defined in `main.py` or elsewhere. Provide invalid YAML files (missing keys, wrong types, invalid dependencies). **Verify:** Does it catch and report errors clearly? Does it prevent the application from running with invalid config?
    * **Test 8:** Rerun `main.py` with various valid YAML configurations defining different workflows. **Verify:** Does the application execute the workflow defined in the specific YAML file loaded for that run? Debug YAML parsing, dynamic object creation, dependency resolution, and validation logic.

This exhaustive plan provides maximum context for each step, emphasizing verification and incremental progress to manage the complexity of integrating the verified simple components into the final, sophisticated application.
